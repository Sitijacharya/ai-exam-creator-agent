"""
Complete Streamlit UI for exam generation with FULL RAG evaluation
UPDATED: Integrates evaluation/rag_evaluator.py for advanced metrics

Replace your entire ui/app.py with this file
"""

import sys
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

import streamlit as st
from typing import List, Tuple
import json
import math
import re

from config.settings import CHROMA_DB_DIR, COLLECTION_NAME, TOP_K_CONTEXTS
from vectorstore.embeddings import EmbeddingManager
from vectorstore.store import VectorStore
from generation.retriever import ContextRetriever
from generation.question_generator import QuestionGenerator
from models.schemas import QuestionBlueprint, ExamConfig, Question

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Try to import RAG evaluator (optional - falls back to basic evaluation if not available)
try:
    from evaluation.rag_evaluator import RAGEvaluator
    EVALUATOR_AVAILABLE = True
    logger.info("‚úì RAG Evaluator loaded successfully")
except ImportError:
    EVALUATOR_AVAILABLE = False
    logger.warning("‚ö†Ô∏è RAG Evaluator not found - using basic evaluation only")

# Page config
st.set_page_config(
    page_title="AI Exam Creator - Production RAG",
    page_icon="üéì",
    layout="wide"
)

def initialize_system():
    """Initialize the RAG system"""
    if "system_initialized" not in st.session_state:
        with st.spinner("üîÑ Initializing RAG system..."):
            try:
                # Load embeddings
                embedding_manager = EmbeddingManager()
                embeddings = embedding_manager.get_embeddings()
                
                # Load vectorstore
                vectorstore_manager = VectorStore(
                    persist_directory=str(CHROMA_DB_DIR),
                    embeddings=embeddings,
                    collection_name=COLLECTION_NAME
                )
                vectorstore = vectorstore_manager.load_vectorstore()
                
                # Initialize retriever with relevance threshold
                retriever = ContextRetriever(
                    vectorstore, 
                    relevance_threshold=0.5,
                    use_reranking=True  # NEW: Enable reranking
                )
                generator = QuestionGenerator()
                
                # Initialize evaluator if available
                if EVALUATOR_AVAILABLE:
                    evaluator = RAGEvaluator()
                    st.session_state["evaluator"] = evaluator
                
                # Store in session state
                st.session_state["vectorstore_manager"] = vectorstore_manager
                st.session_state["retriever"] = retriever
                st.session_state["generator"] = generator
                st.session_state["system_initialized"] = True
                
                logger.info("‚úì System initialized successfully")
                
            except Exception as e:
                st.error(f"‚ùå Failed to initialize system: {e}")
                st.info("Make sure you've run: python scripts/setup_knowledge_base.py")
                st.stop()

def extract_topics_with_weights(syllabus_text: str) -> List[Tuple[str, float]]:
    """Extract topics with credit hours/weights from syllabus text"""
    topics_with_weights = []
    lines = syllabus_text.split('\n')
    
    for line in lines:
        line = line.strip()
        
        if len(line) < 1:
            continue
        
        weight_match = re.search(r'[\(\[](\d+(?:\.\d+)?)\s*(?:hr|hrs|hour|hours)?[\)\]]', line, re.IGNORECASE)
        
        if weight_match:
            weight = float(weight_match.group(1))
            topic = re.sub(r'[\(\[](\d+(?:\.\d+)?)\s*(?:hr|hrs|hour|hours)?[\)\]]', '', line, flags=re.IGNORECASE)
        else:
            weight = 1.0
            topic = line
        
        topic = re.sub(r'^[‚Ä¢\-*\d\.]+\s*', '', topic)
        topic = re.sub(r'^Unit\s*\d+\s*:?\s*', '', topic, flags=re.IGNORECASE)
        topic = topic.strip()
        
        if len(topic) >= 2:
            topics_with_weights.append((topic, weight))
    
    return topics_with_weights[:20]

def calculate_weighted_distribution(topics_with_weights: List[Tuple[str, float]], total_questions: int) -> List[Tuple[str, float, int]]:
    """Calculate how many questions each topic should get based on weights"""
    if not topics_with_weights:
        return []
    
    total_weight = sum(weight for _, weight in topics_with_weights)
    
    distribution = []
    allocated = 0
    
    for i, (topic, weight) in enumerate(topics_with_weights):
        if i == len(topics_with_weights) - 1:
            questions = total_questions - allocated
        else:
            questions = round((weight / total_weight) * total_questions)
        
        allocated += questions
        distribution.append((topic, weight, questions))
    
    return distribution

def main():
    st.title(" AI Exam Creator - RAG System")
    
    st.markdown("""
    ### How it works:
    1.  **Knowledge Base**: PDFs are pre-processed and stored in ChromaDB
    2.  **Syllabus Input**: Provide topics with credit hours (optional)
    3.  **Weighted Distribution**: Questions distributed by credit hours
    4.  **Smart Retrieval**: Finds relevant contexts when available
    5.  **Flexible Generation**: Uses context OR generates from topic knowledge
    6.  **Advanced Evaluation**: RAGAS metrics to assess exam quality
    """)
    
    # Initialize system
    initialize_system()
    
    # Get system components
    vectorstore_manager = st.session_state["vectorstore_manager"]
    retriever = st.session_state["retriever"]
    generator = st.session_state["generator"]
    
    # Sidebar: System stats
    with st.sidebar:
            st.header("üìä System Status")
            
            stats = vectorstore_manager.get_statistics()
            st.metric("Total Vectors", f"{stats.get('total_vectors', 0):,}")
            st.metric("Embedding Dimensions", stats.get('embedding_dimensions', 'N/A'))
            st.info(f"**Collection:** {stats.get('collection_name', 'N/A')}")
            
            st.markdown("---")
            st.markdown("### ‚öôÔ∏è Relevance Settings")
            
            relevance_threshold = st.slider(
                "Context Relevance Threshold",
                min_value=0.0,
                max_value=1.0,
                value=0.5,
                step=0.05,
                help="""
                Controls how relevant retrieved content must be:
                ‚Ä¢ 0.3-0.4: Lenient (accepts broader matches)
                ‚Ä¢ 0.5-0.6: Moderate (recommended)
                ‚Ä¢ 0.7-0.8: Strict (only very relevant content)
                """
            )
            
            if retriever.relevance_threshold != relevance_threshold:
                retriever.relevance_threshold = relevance_threshold
                st.success(f"‚úì Threshold updated to {relevance_threshold:.2f}")

        



        
            
            
            
            # NEW: Reranking toggle
            st.markdown("#### üéØ Reranking")
            
            use_reranking = st.checkbox(
                "Enable Cross-Encoder Reranking",
                value=True,
                help="""
                Cross-Encoder reranking significantly improves context quality by:
                ‚Ä¢ Filtering out irrelevant results
                ‚Ä¢ Reordering by true relevance
                ‚Ä¢ Reducing hallucination
                
                Impact: +15-20% accuracy
                Speed: ~100ms additional latency
                """
            )
            
            # Update retriever if toggle changed
            if use_reranking != retriever.use_reranking:
                retriever.use_reranking = use_reranking
                if use_reranking and not retriever.reranker:
                    try:
                        from generation.reranker import ContextReranker
                        retriever.reranker = ContextReranker()
                        st.success("‚úÖ Reranking enabled")
                    except Exception as e:
                        st.error(f"Failed to enable reranking: {e}")
                        retriever.use_reranking = False
                elif not use_reranking:
                    st.info("‚ÑπÔ∏è Reranking disabled")
            
            # Show reranking status
            if retriever.use_reranking:
                st.success("üéØ Reranking: **Active**")
            else:
                st.warning("‚ö†Ô∏è Reranking: **Disabled**")
            
            st.markdown("---")
            st.markdown("### üí° Features")
            st.success("‚úÖ Extracts ALL topics")
            st.success("‚úÖ Weight-based distribution")
            st.success("‚úÖ Relevance filtering")
            if retriever.use_reranking:
                st.success("‚úÖ **Cross-Encoder reranking**")
            if EVALUATOR_AVAILABLE:
                st.success("‚úÖ Advanced RAG evaluation")
                
                
            st.markdown("---")
            st.markdown("### üìö Format Examples")
            st.code("""1. ML (7 hr)
    2. SQL (3 hrs)
    3. Python (5 hours)
    4. AI [4hr]
    5. Data Science""")
        
        # Main content
    tab1, tab2, tab3 = st.tabs(["üìã Create Exam", "üìñ View Generated Exam", "üìä Evaluation"])
        
    with tab1:
            st.header("Step 1: Provide Syllabus")
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                syllabus_text = st.text_area(
                    "Enter syllabus topics (one per line, credit hours optional)",
                    height=300,
                    placeholder="""1. supervised learning (7 hr)
    2. sql (3 hr)
    3. tender
    4. Machine Learning (5 hrs)
    5. Deep Learning (4 hours)""",
                    help="Format: Topic name (credit hours). Short names like 'SQL', 'AI' work too!",
                    key="syllabus_input"
                )
                
                process_button = st.button("üìã Process Syllabus", type="secondary", use_container_width=True)
            
            with col2:
                st.markdown("**Tips:**")
                st.markdown("- Add credit hours: `(7 hr)`")
                st.markdown("- Works for ANY topic")
                st.markdown("- Questions distributed by weight")
            
            if "extracted_topics" not in st.session_state:
                st.session_state["extracted_topics"] = []
            
            if process_button and syllabus_text.strip():
                topics_with_weights = extract_topics_with_weights(syllabus_text)
                st.session_state["extracted_topics"] = topics_with_weights
                
                if topics_with_weights:
                    st.success(f"‚úÖ Extracted {len(topics_with_weights)} topics")
                else:
                    st.warning("‚ö†Ô∏è No topics found. Please check your format.")
            
            if st.session_state["extracted_topics"]:
                topics_with_weights = st.session_state["extracted_topics"]
                
                with st.expander("üìö Extracted Topics & Weights", expanded=True):
                    total_weight = sum(w for _, w in topics_with_weights)
                    
                    st.markdown("| # | Topic | Credit Hours | Weight % |")
                    st.markdown("|---|-------|--------------|----------|")
                    for i, (topic, weight) in enumerate(topics_with_weights, 1):
                        weight_percent = (weight / total_weight) * 100
                        st.markdown(f"| {i} | {topic} | {weight} | {weight_percent:.1f}% |")
                    
                    st.info(f"üìä Total credit hours: {total_weight}")
                
                st.markdown("---")
                st.header("Step 2: Configure Exam Blueprint")
                
                exam_title = st.text_input("Exam Title", value="Summative Assessment")
                exam_instructions = st.text_area(
                    "Exam Instructions",
                    value="Answer all questions. Each question is based on your course materials.",
                    height=80
                )
                
                st.subheader("Question Types")
                st.info("üí° Questions will be distributed across topics based on their credit hours")
                
                blueprints = []
                
                # MCQ
                with st.expander("üìù Multiple Choice Questions (MCQ)"):
                    col1, col2, col3 = st.columns(3)
                    mcq_count = col1.number_input("Total Count", 0, 100, 10, key="mcq_count")
                    mcq_diff = col2.selectbox("Difficulty", ["Easy", "Moderate", "Hard"], index=1, key="mcq_diff")
                    mcq_marks = col3.number_input("Marks", 1, 10, 1, key="mcq_marks")
                    
                    if mcq_count > 0:
                        blueprints.append(QuestionBlueprint(
                            question_type="Multiple Choice",
                            count=mcq_count,
                            difficulty=mcq_diff,
                            marks_per_question=mcq_marks
                        ))
                        
                        distribution = calculate_weighted_distribution(topics_with_weights, mcq_count)
                        st.markdown("**Distribution Preview:**")
                        for topic, weight, q_count in distribution:
                            if q_count > 0:
                                st.caption(f"  ‚Ä¢ {topic}: {q_count} MCQs (weight: {weight})")
                
                # True/False
                with st.expander("‚úîÔ∏è True/False Questions"):
                    col1, col2, col3 = st.columns(3)
                    tf_count = col1.number_input("Total Count", 0, 100, 5, key="tf_count")
                    tf_diff = col2.selectbox("Difficulty", ["Easy", "Moderate", "Hard"], index=0, key="tf_diff")
                    tf_marks = col3.number_input("Marks", 1, 10, 1, key="tf_marks")
                    
                    if tf_count > 0:
                        blueprints.append(QuestionBlueprint(
                            question_type="True/False",
                            count=tf_count,
                            difficulty=tf_diff,
                            marks_per_question=tf_marks
                        ))
                
                # Short Answer
                with st.expander("üìÑ Short Answer Questions"):
                    col1, col2, col3 = st.columns(3)
                    sa_count = col1.number_input("Total Count", 0, 100, 5, key="sa_count")
                    sa_diff = col2.selectbox("Difficulty", ["Easy", "Moderate", "Hard"], index=1, key="sa_diff")
                    sa_marks = col3.number_input("Marks", 1, 10, 2, key="sa_marks")
                    
                    if sa_count > 0:
                        blueprints.append(QuestionBlueprint(
                            question_type="Short Answer",
                            count=sa_count,
                            difficulty=sa_diff,
                            marks_per_question=sa_marks
                        ))
                
                # Long Answer
                with st.expander("üìã Long Answer Questions"):
                    col1, col2, col3 = st.columns(3)
                    la_count = col1.number_input("Total Count", 0, 100, 2, key="la_count")
                    la_diff = col2.selectbox("Difficulty", ["Easy", "Moderate", "Hard"], index=2, key="la_diff")
                    la_marks = col3.number_input("Marks", 1, 20, 5, key="la_marks")
                    
                    if la_count > 0:
                        blueprints.append(QuestionBlueprint(
                            question_type="Long Answer",
                            count=la_count,
                            difficulty=la_diff,
                            marks_per_question=la_marks
                        ))
                
                st.markdown("---")
                
                if st.button("üöÄ Generate Exam", type="primary", use_container_width=True):
                    if not blueprints:
                        st.error("‚ùå Please configure at least one question type")
                    else:
                        generate_exam_weighted(topics_with_weights, blueprints, exam_title, exam_instructions, retriever, generator)
            else:
                st.info("üëÜ Enter your syllabus topics above and click 'Process Syllabus' to continue")
        
    with tab2:
            if "generated_questions" in st.session_state:
                display_generated_exam()
            else:
                st.info("üëà Generate an exam first in the 'Create Exam' tab")
        
    with tab3:
            if "generated_questions" in st.session_state:
                show_evaluation_dashboard()
            else:
                st.info("üëà Generate an exam first to see evaluation metrics")



def generate_exam_weighted(topics_with_weights, blueprints, title, instructions, retriever, generator):
    """Generate complete exam with weighted question distribution"""
    all_questions = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_questions_needed = sum(bp.count for bp in blueprints)
    questions_generated = 0
    
    questions_with_context = 0
    questions_without_context = 0
    
    st.info(f"üìù Generating {total_questions_needed} total questions...")
    
    for blueprint in blueprints:
        st.markdown(f"### Processing: {blueprint.question_type} ({blueprint.count} questions)")
        
        distribution = calculate_weighted_distribution(topics_with_weights, blueprint.count)
        
        for topic, weight, q_count in distribution:
            if q_count <= 0:
                continue
            
            with st.spinner(f"üîç {topic} ({q_count} questions)"):
                contexts = retriever.retrieve_for_topic(topic, k=TOP_K_CONTEXTS)
                
                if not contexts:
                    st.warning(f"‚ö†Ô∏è No KB content for: {topic}")
                    st.info(f"ü§ñ Using general knowledge")
                else:
                    st.success(f"‚úì Found {len(contexts)} contexts")
                
                topic_blueprint = QuestionBlueprint(
                    question_type=blueprint.question_type,
                    count=q_count,
                    difficulty=blueprint.difficulty,
                    marks_per_question=blueprint.marks_per_question
                )
                
                questions = generator.generate_questions(topic, topic_blueprint, contexts)
                
                if contexts:
                    questions_with_context += len(questions)
                else:
                    questions_without_context += len(questions)
                
                questions = questions[:q_count]
                all_questions.extend(questions)
                
                questions_generated += len(questions)
                
                progress = questions_generated / total_questions_needed
                progress_bar.progress(min(progress, 1.0))
                status_text.text(f"Generated {questions_generated}/{total_questions_needed}")
        
        st.markdown("---")
    
    if all_questions:
        st.session_state["generated_questions"] = all_questions
        st.session_state["exam_title"] = title
        st.session_state["exam_instructions"] = instructions
        
        st.success(f"üéâ Successfully generated {len(all_questions)} questions!")
        
        col1, col2, col3 = st.columns(3)
        col1.metric("Total", len(all_questions))
        col2.metric("From KB", questions_with_context)
        col3.metric("From LLM", questions_without_context)
        
        
    else:
        st.error("‚ùå No questions generated")

def display_generated_exam():
    """Display the generated exam"""
    questions = st.session_state["generated_questions"]
    title = st.session_state.get("exam_title", "Exam")
    instructions = st.session_state.get("exam_instructions", "")
    
    st.title(f"üìÑ {title}")
    st.info(instructions)
    
    # Statistics
    col1, col2, col3, col4 = st.columns(4)
    
    type_counts = {}
    topic_counts = {}
    total_marks = 0
    questions_with_sources = 0
    
    for q in questions:
        type_counts[q.type] = type_counts.get(q.type, 0) + 1
        topic_counts[q.topic] = topic_counts.get(q.topic, 0) + 1
        total_marks += q.marks
        if q.retrieved_contexts:
            questions_with_sources += 1
    
    col1.metric("Questions", len(questions))
    col2.metric("Marks", total_marks)
    col3.metric("Topics", len(topic_counts))
    col4.metric("With Sources", questions_with_sources)
    
    st.markdown("---")
    
    # Group by type
    questions_by_type = {}
    for q in questions:
        if q.type not in questions_by_type:
            questions_by_type[q.type] = []
        questions_by_type[q.type].append(q)
    
    # Display questions
    for qtype, qs in questions_by_type.items():
        st.header(f"üìù {qtype} ({len(qs)} questions)")
        
        for i, q in enumerate(qs, 1):
            with st.container():
                st.markdown(f"### Question {i} - {q.topic}")
                st.markdown(f"**Difficulty:** {q.difficulty} | **Marks:** {q.marks}")
                
                if q.retrieved_contexts:
                    st.caption("üìö From knowledge base")
                else:
                    st.caption("ü§ñ From general knowledge")
                
                st.markdown(q.prompt)
                
                if q.choices:
                    for choice in q.choices:
                        st.markdown(f"**{choice.label}.** {choice.text}")
                
                if q.retrieved_contexts:
                    with st.expander("üîç View Source Context"):
                        for idx, ctx in enumerate(q.retrieved_contexts, 1):
                            relevance_pct = ctx.relevance_score * 100
                            
                            if ctx.relevance_score >= 0.7:
                                relevance_color = "üü¢"
                            elif ctx.relevance_score >= 0.5:
                                relevance_color = "üü°"
                            else:
                                relevance_color = "üî¥"
                            
                            st.markdown(f"**üìñ Source {idx}:** {ctx.metadata['source_file']} (Page {ctx.metadata['page']})")
                            st.info(ctx.metadata['preview'])
                            st.caption(f"{relevance_color} Relevance: {relevance_pct:.1f}%")
                else:
                    with st.expander("‚ÑπÔ∏è Generation Info"):
                        st.info(f"Generated using AI's general knowledge about '{q.topic}'")
                
                st.markdown("---")
    
    # Answer key
    st.header("üîë Answer Key")
    for qtype, qs in questions_by_type.items():
        st.subheader(qtype)
        for i, q in enumerate(qs, 1):
            with st.expander(f"Q{i}: {q.prompt[:80]}..."):
                st.markdown(f"**Topic:** {q.topic}")
                st.markdown(f"**Answer:** {q.answer}")
                if q.explanation:
                    st.markdown(f"**Explanation:** {q.explanation}")
    
    # Download
    st.markdown("---")
    st.header("üíæ Download")
    
    col1, col2 = st.columns(2)
    
    with col1:
        export_data = {
            "title": title,
            "instructions": instructions,
            "questions": [q.dict() for q in questions]
        }
        json_str = json.dumps(export_data, indent=2)
        
        st.download_button(
            "üì• JSON",
            json_str,
            f"{title.replace(' ', '_')}.json",
            "application/json"
        )
    
    with col2:
        text_parts = [f"{title}\n{'='*len(title)}\n\n{instructions}\n\n"]
        
        for qtype, qs in questions_by_type.items():
            text_parts.append(f"\n{qtype}\n{'-'*len(qtype)}\n\n")
            for i, q in enumerate(qs, 1):
                text_parts.append(f"Q{i}. [{q.topic}] {q.prompt}\n")
                if q.choices:
                    for choice in q.choices:
                        text_parts.append(f"  {choice.label}. {choice.text}\n")
                text_parts.append("\n")
        
        text_content = "".join(text_parts)
        
        st.download_button(
            "üì• Text",
            text_content,
            f"{title.replace(' ', '_')}.txt",
            "text/plain"
        )

def show_evaluation_dashboard():
    """Display RAG evaluation dashboard with advanced metrics"""
    st.header("üìä RAG Evaluation Dashboard")
    
    questions = st.session_state.get("generated_questions", [])
    
    if not questions:
        st.warning("No questions to evaluate")
        return
    
    # Check if advanced evaluator is available
    if EVALUATOR_AVAILABLE and "evaluator" in st.session_state:
        show_advanced_evaluation(questions)
    

def show_advanced_evaluation(questions):
    """Advanced evaluation using RAG Evaluator"""
    evaluator = st.session_state["evaluator"]
    
    st.info("üöÄ Using Advanced RAG Evaluation (RAGAS Framework)")
    
    # Evaluation options
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.markdown("Evaluating exam quality using industry-standard RAGAS metrics...")
    
    with col2:
        sample_size = st.selectbox(
            "Sample Size",
            [5, 10, 20, "All"],
            index=1,
            help="Number of questions to evaluate (smaller = faster)"
        )
    
    if st.button("üîç Run Advanced Evaluation", type="primary"):
        with st.spinner("Evaluating questions... This may take a minute..."):
            # Determine sample size
            if sample_size == "All":
                eval_sample = None
            else:
                eval_sample = int(sample_size)
            
            # Run batch evaluation
            results = evaluator.evaluate_exam_batch(questions, sample_size=eval_sample)
            
            # Store results
            st.session_state["evaluation_results"] = results
    
    # Display results if available
    if "evaluation_results" in st.session_state:
        results = st.session_state["evaluation_results"]
        summary = results["summary"]
        
        st.markdown("---")
        st.markdown("### üéØ Overall RAGAS Score")
        
        ragas_score = summary["overall_ragas_score"]
        
        # Determine grade
        if ragas_score >= 0.8:
            grade = "A"
            color = "üü¢"
            status = "Excellent"
        elif ragas_score >= 0.7:
            grade = "B"
            color = "üü°"
            status = "Good"
        elif ragas_score >= 0.6:
            grade = "C"
            color = "üü†"
            status = "Fair"
        else:
            grade = "D"
            color = "üî¥"
            status = "Needs Improvement"
        
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            st.metric("RAGAS Score", f"{ragas_score:.1%}", help="Overall RAG quality (target: ‚â•70%)")
        
        with col2:
            st.metric("Grade", f"{color} {grade}")
        
        with col3:
            st.metric("Status", status)
        
        # Component scores
        st.markdown("### üìà Component Metrics")
        
        col1, col2, col3 = st.columns(3)
        
        col1.metric(
            "Faithfulness",
            f"{summary['avg_faithfulness']:.1%}",
            help="Are questions grounded in context? (target: ‚â•90%)"
        )
        
        col2.metric(
            "Relevance",
            f"{summary['avg_relevance']:.1%}",
            help="Do questions test the intended topic? (target: ‚â•80%)"
        )
        
        col3.metric(
            "Context Usage",
            f"{summary['context_usage_rate']:.1%}",
            help="% questions using textbook content"
        )
        
        # Score distribution
        st.markdown("### üìä Score Distribution")
        
        dist = summary["score_distribution"]
        
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Excellent (A)", dist["excellent (‚â•0.8)"])
        col2.metric("Good (B)", dist["good (0.7-0.8)"])
        col3.metric("Fair (C)", dist["fair (0.6-0.7)"])
        col4.metric("Poor (D-F)", dist["poor (<0.6)"])
        

       
        
        
        # Detailed results
        with st.expander("üìã Detailed Question Scores"):
            for result in results["detailed_results"][:10]:
                ragas = result["ragas"]
                
                st.markdown(f"**{result['topic']}** ({result['type']})")
                st.progress(ragas["ragas_score"])
                st.caption(f"Score: {ragas['ragas_score']:.1%} - {ragas['interpretation']}")
                st.markdown("---")
        
        # Export report
        st.markdown("### üíæ Export Report")
        
        report_json = json.dumps(results, indent=2)
        
        st.download_button(
            "üì• Download Evaluation Report",
            report_json,
            "ragas_evaluation_report.json",
            "application/json"
        )



if __name__ == "__main__":
    main()